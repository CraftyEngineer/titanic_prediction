# ğŸš¢ Titanic - Machine Learning from Disaster

Welcome to my solution for the **Titanic - Machine Learning from Disaster** Kaggle competition!  
In this project, I built multiple models to predict passenger survival based on various features such as age, gender, class, family size, and more.
Ranked 3800 out of 15985

---

## ğŸ§  Objective

The goal of this notebook is to predict whether a passenger survived the Titanic disaster using the provided dataset. This is a classic **binary classification problem**, where the target variable is:

- `Survived` = `0` (Did not survive)
- `Survived` = `1` (Survived)

---

## ğŸ“ Dataset Used

- `train.csv` â€“ Training data used for model training and validation
- `test.csv` â€“ Data for which survival prediction is required

---

## ğŸ§¹ Data Preprocessing & Feature Engineering

Before building the models, I performed the following preprocessing steps:

### ğŸ”§ Cleaning and Encoding:
- Converted **Sex** to numerical (`male` = 0, `female` = 1)
- Converted **Embarked** values to numerical (`S` = 0, `C` = 1, `Q` = 2)
- Filled missing values in **Age** using **Title-based median age imputation**
- Created **Title** feature from the passengerâ€™s name
- Created **HasCabin** feature to indicate cabin info presence
- Created **FamilySize** = `SibSp + Parch + 1`
- Created **IsAlone** = 1 if `FamilySize == 1` else 0
- Applied **StandardScaler** to scale all features

---

# ğŸ“Š Model Performance & Evaluation Summary

---

## ğŸ”¹ 1. SVM Classifier

- ğŸ”¥ **Training Accuracy:** 79.07%  
- ğŸ§ª **Validation Accuracy:** 78.77%  
- ğŸ§¾ **Kaggle Test Score:** 77.511%

## ğŸ”¹ 2. Random Forest

- ğŸ”¥ **Training Accuracy:** 87.22% 
- ğŸ§ª **Validation Accuracy:** 78.77%  
- ğŸ§¾ **Kaggle Test Score:** 77.511%

## ğŸ”¹ 3. XGBoost Classifier ğŸ¥‡

- ğŸ”¥ **Training Accuracy:** 86.10% 
- ğŸ§ª **Validation Accuracy:** 81.01% 
- ğŸ§¾ **Kaggle Test Score:** 77.990%
âœ… This gave the best public score out of all models!

## ğŸ”¹ 4. Neural Network (Keras Sequential)

- ğŸ”¥ **Training Accuracy:** 86.23% 
- ğŸ§ª **Validation Accuracy:** 82.68% 
- ğŸ§¾ **Kaggle Test Score:** 77.511%
(Yes, despite high val accuracy, public score stayed the same â€“ likely due to overfitting or data distribution shift)

---

## ğŸ“ˆ Evaluation Strategy
- Used train_test_split with stratify=Y to maintain class balance.
- Evaluation metric: Accuracy
- Predicted test results saved as: titanic_<model>_test_predictions.csv

---

## ğŸ“Š Summary Table

| Model           | Training Acc | Validation Acc | Kaggle Score |
|----------------|--------------|----------------|--------------|
| SVM            | 79.07%       | 78.77%         | 77.511%      |
| Random Forest  | 87.22%       | 78.77%         | 77.511%      |
| Neural Network | 86.23%       | 82.68%         | 77.511%      |
| XGBoost        | 86.10%       | 81.01%         | ğŸ¥‡ 77.990%    |

---

## ğŸ§  Lessons Learned
- Simpler models like SVM and Random Forest can perform well with good feature engineering.
- More complex models like Neural Nets may overfit without careful tuning.
- XGBoost consistently delivers strong results for tabular data.
- Public leaderboard performance doesn't always align with validation accuracy!
  
---

## ğŸ›  Future Improvements
- Use cross-validation instead of a single train/val split
- Tune hyperparameters (e.g., using GridSearchCV or Optuna)
- Try ensemble techniques (stacking/blending)
- Handle outliers and apply PCA for dimensionality reduction

---
## ğŸ’¬ Feedback?
Have suggestions or questions?
Feel free to open an issue or message me!

---

## â­ Credits
Huge thanks to Kaggle and the Titanic dataset contributors.
Notebook created with â¤ï¸ by CraftyEngineer

---

## ğŸ“ License
This repository is open source and available under the MIT License.
